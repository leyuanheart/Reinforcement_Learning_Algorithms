{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5460f9",
   "metadata": {},
   "source": [
    "![ce](https://pic.imgdb.cn/item/649a59261ddac507cc728dbe.png)\n",
    "\n",
    "**reference**: https://github.com/ucla-rlcourse/RLexample/blob/master/my_learning_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9500fb9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T07:40:39.669216Z",
     "start_time": "2023-06-27T07:40:39.347558Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f5cf64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T07:44:42.822193Z",
     "start_time": "2023-06-27T07:44:42.803991Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryActionLinearPolicy(object):\n",
    "    def __init__(self, theta):\n",
    "        # theta dim: (obs_dim + 1, )\n",
    "        self.w = theta[:-1]\n",
    "        self.b = theta[-1]\n",
    "    def act(self, ob):\n",
    "        y = ob.dot(self.w) + self.b\n",
    "        a = int(y < 0)    # 本质和对y施加一个logistic函数，然后判断和0.5的大小是一样的，这样可以简化计算，加快速度。\n",
    "        return a\n",
    "    \n",
    "    \n",
    "class ContinuousActionLinearPolicy(object):\n",
    "    def __init__(self, theta, obs_dim, action_dim):\n",
    "        assert len(theta) == (obs_dim + 1) * action_dim\n",
    "        self.W = theta[:obs_dim * action_dim].reshape(obs_dim, action_dim)\n",
    "        self.b = theta[obs_dim * action_dim:].reshape(1, action_dim)\n",
    "    def act(self, ob):\n",
    "        a = ob.dot(self.W) + self.b  # 如果有action有范围限制，可以加上一个tanh函数来调节输出的范围\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa787312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T07:44:44.810000Z",
     "start_time": "2023-06-27T07:44:44.803000Z"
    }
   },
   "outputs": [],
   "source": [
    "def cem(J, mu, N, elite_frac, th_std):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "\n",
    "    J: a function mapping from vector -> scalar\n",
    "    mu: initial mean over input distribution, dim: [(obs_dim+1) * action_dim]\n",
    "    N: number of samples of theta to evaluate per batch\n",
    "    elite_frac: each batch, select this fraction of the top-performing samples\n",
    "    th_std: istandard deviation over parameter vectors\n",
    "    \"\"\"\n",
    "    n_elite = int(np.round(N * elite_frac))\n",
    "    \n",
    "    # sample theta's from a gaussian distribution with th_mean and th_std\n",
    "    thetas = np.array([mu + std for std in th_std[None, :] * np.random.randn(N, mu.size)])\n",
    "    # evaluate each theta using J(theta)\n",
    "    Js = np.array([J(theta) for theta in thetas])\n",
    "    # select thetas with the top n_elite performances\n",
    "    elite_inds = Js.argsort()[::-1][:n_elite]\n",
    "    elite_thetas = thetas[elite_inds]\n",
    "    # update mu and std\n",
    "    mu = elite_thetas.mean(axis=0)\n",
    "    th_std = elite_thetas.std(axis=0)\n",
    "    \n",
    "    return Js.mean(), mu, th_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b39ae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T07:44:49.754970Z",
     "start_time": "2023-06-27T07:44:49.746993Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_rollout(agent, env):\n",
    "    total_rewards = 0\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(1000):\n",
    "        a = agent.act(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(a)\n",
    "        total_rewards += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    return total_rewards\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    # J(theta)\n",
    "    agent = BinaryActionLinearPolicy(theta)\n",
    "    total_rewards = do_rollout(agent, env)\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac235c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c3851fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T07:57:29.106929Z",
     "start_time": "2023-06-27T07:57:29.090841Z"
    }
   },
   "outputs": [],
   "source": [
    "n_iters = 200\n",
    "N = 30\n",
    "elite_frac = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46c096b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T08:01:57.493044Z",
     "start_time": "2023-06-27T08:01:54.643598Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 J means:  51.43333333333333\n",
      "Step:  1 J means:  37.7\n",
      "Step:  2 J means:  69.33333333333333\n",
      "Step:  3 J means:  88.03333333333333\n",
      "Step:  4 J means:  92.5\n",
      "Step:  5 J means:  118.76666666666667\n",
      "Step:  6 J means:  156.5\n",
      "Step:  7 J means:  198.76666666666668\n",
      "Step:  8 J means:  370.06666666666666\n",
      "Step:  9 J means:  380.1666666666667\n",
      "Step:  10 J means:  508.5\n",
      "Step:  11 J means:  614.3333333333334\n",
      "Step:  12 J means:  864.8\n",
      "Step:  13 J means:  912.2666666666667\n",
      "Step:  14 J means:  997.4\n",
      "Step:  15 J means:  971.6333333333333\n",
      "Step:  16 J means:  998.2\n",
      "Step:  17 J means:  985.1333333333333\n",
      "Step:  18 J means:  1000.0\n",
      "Training is done\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', max_episode_steps=1000, render_mode=None)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "action_dim = 1 if len(env.action_space.shape)==0 else env.action_space.shape[0]\n",
    "\n",
    "mu = np.zeros((obs_dim + 1) * action_dim)\n",
    "th_std = np.ones_like(mu)\n",
    "for step in range(n_iters):\n",
    "    J_mean, mu, th_std = cem(noisy_evaluation, mu, N, elite_frac, th_std)\n",
    "    print('Step: ', step, 'J means: ', J_mean)\n",
    "    \n",
    "    if J_mean > 999.999:\n",
    "        print('Training is done') \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b0af3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T08:02:22.691025Z",
     "start_time": "2023-06-27T08:02:01.266730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards:  1000.0 Total steps:  1000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.normal(mu, th_std)\n",
    "agent = BinaryActionLinearPolicy(theta)\n",
    "env = gym.make('CartPole-v1', max_episode_steps=1000, render_mode=\"human\")\n",
    "obs, info = env.reset(seed=42)\n",
    "total_rewards = 0\n",
    "for t in range(1000):\n",
    "    action = agent.act(obs)  # this is where you would insert your policy\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_rewards += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "env.close()\n",
    "print('Total rewards: ', total_rewards, 'Total steps: ', t+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c0356f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T08:02:24.369325Z",
     "start_time": "2023-06-27T08:02:24.355088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01422972, -0.80893479, -1.12050025, -1.47640291,  0.00489943]),\n",
       " array([0.02448865, 0.1431876 , 0.05402537, 0.03403916, 0.01739198]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, th_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9684b942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-27T08:02:26.118002Z",
     "start_time": "2023-06-27T08:02:26.098731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminated, truncated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
